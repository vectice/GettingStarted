{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Vectice and other packages\n",
    "%pip install -q vectice -U\n",
    "%pip install boto3\n",
    "%pip install botocore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Paste your API token below and execute the block. (your token can be generated [here](https://app.vectice.com/account/api-keys) )   \n",
    "\n",
    "Dataset used can be found here: https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/items.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/items.csv -q --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vectice package\n",
    "import vectice as vct\n",
    "from vectice import DatasetType, DatasetSourceUsage, Dataset, FileResource, S3Resource\n",
    "\n",
    "\n",
    "# Connect using your token API - Your token can be found here: https://app.vectice.com/account/api-keys\n",
    "conn = vct.connect(\n",
    "    api_token='YOUR API TOKEN', \n",
    "    host='https://app.vectice.com',\n",
    "    workspace='Samples'\n",
    ")\n",
    "\n",
    "# Optional Vectice flags\n",
    "vct.code_capture = False #ON by default\n",
    "\n",
    "# Open the project\n",
    "#conn = conn.workspace('Samples')\n",
    "project = conn.project(\"How To: Reporting your Milestones\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate methods of connecting:  \n",
    "<ul>project = vct.connect(config='~/.config/vectice-config.json')</ul>  \n",
    "provided the json file contains the \"WORKSPACE\" and \"PROJECT\" entries,  \n",
    "OR  \n",
    "<ul>project = vct.connect(config='~/.config/vectice-config.json', workspace=\"ws_name\", project=\"project_name\")</ul>  \n",
    "Both will return a project object\n",
    "\n",
    "[API Reference](https://api-docs.vectice.com/reference/vectice/connection/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture your dataset and their usage\n",
    "\n",
    "This sample uses data from our Vectice S3 bucket. \n",
    "     \n",
    "We will use boto3 as a client.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3 import client  # Used to create a client and read from S3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "from vectice import FileResource, S3Resource, GCSResource, DatasetSourceUsage\n",
    "\n",
    "s3_client = client('s3', config=Config(signature_version=UNSIGNED), region_name='us-west-1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first cell illustrates how to create a Vectice metadataset object, set it as an origin asset, and add it's lineage to the source of the data    \n",
    "\n",
    "The second cell shows how to tag/attach a clean dataset ready for modeling to your project   \n",
    "\n",
    "The third one captures the definition of you modeling dataset (compound dataset - training, testing, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scientist code to build data frames with data\n",
    "# ...\n",
    "\n",
    "# Let's list out the phases and status - Vectice is bi-directional\n",
    "project.list_phases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instanciate an iteration for the second phase in the list above (Document Dataset)\n",
    "phase_iter = project.phase(\"Document Dataset\").iteration()\n",
    "\n",
    "# Let's have a look at the steps needed to be completed\n",
    "phase_iter.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great I now know the structure of my projects and the items I need to complete\n",
    "\n",
    "# Document the original datasets used for this iteration (step_identify_source_datasets)\n",
    "\n",
    "# Using a S3Resource object for files on AWS S3:\n",
    "phase_iter.step_identify_source_datasets = Dataset.origin( name=\"Stores\", resource=S3Resource(s3_client=s3_client,bucket_name='vectice-examples',resource_path=\"Tutorial/ForecastTutorial/stores.csv\"))\n",
    "phase_iter.step_identify_source_datasets += Dataset.origin(name=\"Transactions\",resource=S3Resource(s3_client=s3_client,bucket_name='vectice-examples',resource_path=\"Tutorial/ForecastTutorial/transactions.csv\"))\n",
    "\n",
    "# Using a FileResource for local files\n",
    "local_file = FileResource(path=\"items.csv\")\n",
    "phase_iter.step_identify_source_datasets += Dataset.origin(name=\"Items\",resource=local_file)\n",
    "\n",
    "# More examples here: https://docs.vectice.com/python-api-docs/how-to-register-datasets\n",
    "\n",
    "\n",
    "#Adding a commnent to the step\n",
    "phase_iter.step_identify_source_datasets = \"The datasets for the project have been identified as \\'stores.csv\\' and \\'transaction.csv'.\\nBoth files are located under the \\'vectice-example' S3 bucket.\\n We also identified a local file to help with feature engineering.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scientist code for data preparation, normalization, etc...\n",
    "# ...\n",
    "\n",
    "# Register the cleaned dataset in Vectice\n",
    "phase_iter.step_prep_datasets = Dataset.clean( name=\"Normalized_Cleaned\", resource=S3Resource(s3_client=s3_client,bucket_name='vectice-examples',resource_path=\"Tutorial/ForecastTutorial/train_clean.csv\"))\n",
    "\n",
    "msg = \"As part of our standard Data Pipeline process we applied the following preparation to our datasets: - Handling of missing data - Applied standard scaler to numerical attributes - Converted categorical data into numerical - Split values in numerical value..\"\n",
    "phase_iter.step_prep_datasets = msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmmmmm, which step am I on again? \n",
    "# Vectice makes it easy to follow your progress\n",
    "phase_iter.list_steps()\n",
    "\n",
    "# Ok, looks like I am working on the \"step_partition_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scientist code to generate training, testing, and validation dataframes\n",
    "# ...\n",
    "\n",
    "# Define testing, training and validation resources\n",
    "train_ds = S3Resource(s3_client=s3_client, bucket_name='vectice-examples', resource_path=\"Tutorial/ForecastTutorial/traindataset.csv\")\n",
    "test_ds = S3Resource(s3_client=s3_client, bucket_name='vectice-examples', resource_path=\"Tutorial/ForecastTutorial/testdataset.csv\")\n",
    "validate_ds = S3Resource(s3_client=s3_client, bucket_name='vectice-examples', resource_path=\"Tutorial/ForecastTutorial/validatedataset.csv\")\n",
    "\n",
    "dataset = Dataset.modeling(\n",
    "    name=\"my modeling dataset\",\n",
    "    training_resource=train_ds,\n",
    "    testing_resource=test_ds,\n",
    "    validation_resource=validate_ds,\n",
    ")\n",
    "phase_iter.step_partition_dataset = dataset\n",
    "# Document the step and automatically attach the dataset\n",
    "phase_iter.step_partition_dataset = \"We split the cleaned dataset in a training, testing and validation datasets. 40% of the data is set aside for testing and our seed to generate repeatable datasets is 42\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
