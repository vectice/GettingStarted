{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸŒŸ Auto-document your work with Vectice - Tutorial Probability of Default Notebook\n",
    "\n",
    "This Vectice Tutorial notebook illustrates how to use Vectice auto-documentation features in a realistic business scenario. We will follow a classic but simplified model training flow to quickly show how Vectice can help you automate your **Model Documentation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"color: #383d41; background-color: #e2e3e5; border-color: #d6d8db\" role=\"alert\">\n",
    "<b> This tutorial showcases a documentation-only project with a single phase as a great way to get started. You also have the ability to define more complex project structures to organize your work in multiple phases and facilitate cross-functional collaboration.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites:\n",
    "Before using this notebook you will need:\n",
    "* An account in Vectice\n",
    "* An API key to connect to Vectice through the APIs\n",
    "\n",
    "\n",
    "### Other Resources\n",
    "*   Vectice Documentation: https://docs.vectice.com/ </br>\n",
    "*   Vectice API documentation: https://api-docs.vectice.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the latest Vectice Python client library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q vectice -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "FKTpQh5HeG-s"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Download the data\n",
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Samples+Data/tutorial_data.csv -q --no-check-certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Get started by configuring the Vectice autolog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, we need to authenticate to the Vectice app:**\n",
    "\n",
    "- Visit the Vectice app to create and copy an API key (cf. https://docs.vectice.com/getting-started/create-an-api-key)\n",
    "\n",
    "- Paste the API key in the code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjNhaJcSeG-t",
    "outputId": "371e58e6-ea41-4513-b3d2-425f3853fea4"
   },
   "outputs": [],
   "source": [
    "import vectice\n",
    "from vectice import autolog\n",
    "\n",
    "autolog.config(api_token=\"your-api-key\", #Paste your API key\n",
    "  host = 'your-host-info',  #Paste your host information\n",
    "  phase = 'your-phase-id', #Paste your Phase Id\n",
    "  prefix = 'mdd') # Optional: a prefix for asset names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"color: #383d41; background-color: #e2e3e5; border-color: #d6d8db\" role=\"alert\">\n",
    "<b> Important information:</b>\n",
    "<li> Vectice Autolog is continuously evolving and we are actively enhancing supported libraries, environments, and functionalities to provide an improved user experience. \n",
    "<li> Be sure to <b>configure autolog at the beginning of your notebook</b>. A late setup may require rerunning previous cells.\n",
    "<br>\n",
    "\n",
    "For detailed information, <b>supported libraries and environments</b>, please consult our [Autolog documentation](https://api-docs.vectice.com/reference/vectice/autolog/).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Retrieve from Vectice the list of requirements needed to complete your documentation directly from your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = autolog.get_connection().browse('your-phase-id').list_requirements(display_print=True) # Paste your Phase Id same as in the previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Your Regular Data Science Work -> Not specific to Vectice\n",
    "\n",
    "In this notebook, we will work on predicting the probability of loan default using a simplified yet complete data science workflow. Here's the plan:\n",
    "\n",
    "- Dataset Loading: \n",
    "  - Load a dataset containing information about loan default applications.\n",
    "  - Select a subset of the data.\n",
    "- Data Preparation:\n",
    "  - Perform small feature engineering tasks.\n",
    "  - Apply scaling to the features.\n",
    "- Model Building and Evaluation:\n",
    "  - Build a logistic regression model to predict the probability of loan default.\n",
    "  - Evaluate the results of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a5e67831-4751-4f11-8e07-527e3e092671",
    "_uuid": "ded520f73b9e94ed47ac2e994a5fb1bcb9093d0f",
    "id": "yauz6m0neG-t"
   },
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "ihF90pQqeG-u",
    "outputId": "76973532-e7b9-4277-a95d-756af8d6878c"
   },
   "outputs": [],
   "source": [
    "## For the baseline model, we are only going to select a subset of columns that would make sense for the business, namely ['SK_ID_CURR','AMT_ANNUITY','AMT_CREDIT','AMT_INCOME_TOTAL','AMT_GOODS_PRICE','CNT_CHILDREN','CNT_FAM_MEMBERS','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\"TARGET\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "selected_columns = ['SK_ID_CURR','AMT_ANNUITY','AMT_CREDIT','AMT_INCOME_TOTAL','AMT_GOODS_PRICE','CNT_CHILDREN','CNT_FAM_MEMBERS',\n",
    "           'DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\"TARGET\"]\n",
    "\n",
    "# Training data\n",
    "path_train = \"/home/jupyterlab/Vectice/Field demos/Banking/current_application_preprocessed.csv\"\n",
    "application_cleaned_baseline = pd.read_csv(path_train)[selected_columns]\n",
    "app_train_feat, app_test_feat = train_test_split(application_cleaned_baseline, test_size=0.15, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate the target variable from the testing set\n",
    "\n",
    "target_variable = 'TARGET'\n",
    "app_test_feat_target = app_test_feat[target_variable]\n",
    "app_test_feat = app_test_feat.drop(target_variable, axis=1)\n",
    "\n",
    "# Print the shapes of the resulting dataframes\n",
    "print('Training data shape: ', app_train_feat.shape)\n",
    "print('Testing shape: ', app_test_feat.shape)\n",
    "print('Testing target shape: ', app_test_feat_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDIEB3zieG-u",
    "outputId": "47b3b7ca-6fea-482d-9ec2-0c7cda227e93"
   },
   "outputs": [],
   "source": [
    "app_train_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "86d1b309-5524-4298-b873-2c1c09eddec6",
    "_uuid": "1b49e667293daabffd8a4b2b6d02cf44bf6a3ba8",
    "id": "XoDuFM19eG-u"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "95627792-157e-457a-88a8-3b3875c7e1d5",
    "_uuid": "46f5bf9a6de52e270aa911ffd895e704da5426ec",
    "id": "KtEi7L-YeG-u"
   },
   "source": [
    "### Label Encoding and One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "70641d4d-1075-4837-8972-e58d70d8f242",
    "_uuid": "ddfaae5c3dcc7ec6bb47a2dffc10d364e8d25355",
    "id": "6-WcdOGLeG-u",
    "outputId": "cbef08cd-6b53-40db-8c80-5261268fc89b"
   },
   "outputs": [],
   "source": [
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "for col in app_train_feat:\n",
    "    if app_train_feat[col].dtype == 'object':\n",
    "        if len(list(app_train_feat[col].unique())) <= 2:\n",
    "            le.fit(app_train_feat[col])\n",
    "            app_train_feat[col] = le.transform(app_train_feat[col])\n",
    "            app_test_feat[col] = le.transform(app_test_feat[col])\n",
    "\n",
    "            le_count += 1\n",
    "\n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0851773b-39fd-4cf0-9a66-e30adeef3e57",
    "_uuid": "6796c6dc793a08e162b6e20c6f185ef37bdf51f3",
    "id": "vf6eQZe9eG-v",
    "outputId": "88b77463-ade7-4630-8e96-f64096ca16fd"
   },
   "outputs": [],
   "source": [
    "## one-hot encoding of categorical variables\n",
    "app_train_feat = pd.get_dummies(app_train_feat)\n",
    "app_test_feat = pd.get_dummies(app_test_feat)\n",
    "train_labels = app_train_feat['TARGET']\n",
    "\n",
    "app_train_feat, app_test_feat = app_train_feat.align(app_test_feat, join = 'inner', axis = 1)\n",
    "\n",
    "app_train_feat['TARGET'] = train_labels\n",
    "\n",
    "\n",
    "app_train_feat['DAYS_EMPLOYED_ANOM'] = app_train_feat[\"DAYS_EMPLOYED\"] == 365243\n",
    "\n",
    "app_train_feat['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "\n",
    "\n",
    "app_test_feat['DAYS_EMPLOYED_ANOM'] = app_test_feat[\"DAYS_EMPLOYED\"] == 365243\n",
    "app_test_feat[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n",
    "print('Training Features shape: ', app_train_feat.shape)\n",
    "print('Testing Features shape: ', app_test_feat.shape)\n",
    "print('There are %d anomalies in the test data out of %d entries' % (app_test_feat[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test_feat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and missing Data handling\n",
    "\n",
    "### Define the feature list - drop target and fit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izVPFh52eG-v",
    "outputId": "55cca63b-8244-4ded-8ba9-fdf1054c3d9d"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Drop the target from the training data\n",
    "if 'TARGET' in app_train_feat:\n",
    "    train_no_missing = app_train_feat.drop(columns=['TARGET'])\n",
    "\n",
    "# Separate 'SK_ID_CURR'\n",
    "train_ids = train_no_missing['SK_ID_CURR']\n",
    "test_ids = app_test_feat['SK_ID_CURR']\n",
    "\n",
    "# Drop 'SK_ID_CURR' from the features list\n",
    "train_no_missing = train_no_missing.drop(columns=['SK_ID_CURR'])\n",
    "test_no_missing = app_test_feat.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "# Define the features list without 'SK_ID_CURR'\n",
    "features = list(train_no_missing.columns)\n",
    "\n",
    "## Median imputation of missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "# Fit on the training data\n",
    "imputer.fit(train_no_missing)\n",
    "\n",
    "# Transform both training and testing data\n",
    "train_no_missing = pd.DataFrame(imputer.transform(train_no_missing), columns=features)\n",
    "test_no_missing = pd.DataFrame(imputer.transform(test_no_missing), columns=features)\n",
    "\n",
    "## Standardize the features\n",
    "scaler = StandardScaler()\n",
    "# Fit on the training data\n",
    "scaler.fit(train_no_missing)\n",
    "\n",
    "# Transform both training and testing data\n",
    "train_no_missing = pd.DataFrame(scaler.transform(train_no_missing), columns=features)\n",
    "test_no_missing = pd.DataFrame(scaler.transform(test_no_missing), columns=features)\n",
    "\n",
    "# Reattach 'SK_ID_CURR' to the DataFrames\n",
    "train_no_missing['SK_ID_CURR'] = train_ids.values\n",
    "test_no_missing['SK_ID_CURR'] = test_ids.values\n",
    "\n",
    "# Set 'SK_ID_CURR' as the index\n",
    "train_no_missing = train_no_missing.set_index('SK_ID_CURR')\n",
    "test_no_missing = test_no_missing.set_index('SK_ID_CURR')\n",
    "\n",
    "# Display the first few rows of the transformed training data\n",
    "print(train_no_missing.head())\n",
    "print(test_no_missing.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnFFx0z-eG-v"
   },
   "source": [
    "# Model building and evaluation\n",
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrCueQ_JeG-v",
    "outputId": "1220d525-1e6c-42f2-e39c-751c92d053ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Build a logistic regression model\n",
    "# Define and train the logistic regression model\n",
    "logistic_regression = LogisticRegression(random_state=50, solver='liblinear', max_iter=1000)\n",
    "features = list(train_no_missing.columns)\n",
    "# Train on the training data\n",
    "logistic_regression.fit(train_no_missing, train_labels)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = logistic_regression.predict_proba(test_no_missing)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc = roc_auc_score(app_test_feat_target.values, predictions)\n",
    "\n",
    "sorted_indices = np.argsort(predictions)[::-1]\n",
    "sorted_labels = app_test_feat_target.iloc[sorted_indices]\n",
    "\n",
    "desired_percentage = 0.25\n",
    "\n",
    "threshold_index = int(desired_percentage * len(predictions))\n",
    "threshold_probability = predictions[sorted_indices[threshold_index]]\n",
    "binary_predictions = (predictions >= threshold_probability).astype(int)\n",
    "\n",
    "# Calculate the recall at the desired percentage\n",
    "recall = recall_score(app_test_feat_target.values, binary_predictions)\n",
    "f1 = f1_score(app_test_feat_target.values, binary_predictions)\n",
    "\n",
    "# Print metrics\n",
    "metric = {\"auc\": float(roc_auc),\n",
    "          f\"recall at {desired_percentage}%\": float(recall),\n",
    "          f\"f1_score at {desired_percentage}%\": float(f1)}\n",
    "\n",
    "print(\"ROC AUC Score:\", roc_auc)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Recall Score:\", recall)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(app_test_feat_target.values, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"Performance_roc_curve.png\")\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame with predicted probabilities and true labels\n",
    "df_results = pd.DataFrame({'Probability': predictions, 'Default': app_test_feat_target.values})\n",
    "\n",
    "# Sort instances based on predicted probabilities\n",
    "df_results = df_results.sort_values(by='Probability', ascending=False)\n",
    "\n",
    "# Divide the sorted instances into quantiles (e.g., deciles)\n",
    "num_quantiles = 10\n",
    "df_results['Quantile'] = pd.qcut(df_results['Probability'], q=num_quantiles, labels=False, duplicates='drop')\n",
    "\n",
    "# Calculate the percentage of defaults in each quantile\n",
    "quantile_defaults = df_results.groupby('Quantile')['Default'].mean() * 100\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(quantile_defaults.index, quantile_defaults.values, color='blue', alpha=0.7)\n",
    "plt.xlabel('Quantile of predicted probabilities')\n",
    "plt.ylabel('Percentage of Defaults')\n",
    "plt.title('Percentage of Defaults by Quantile of Predicted Probabilities')\n",
    "plt.xticks(ticks=quantile_defaults.index, labels=[f'Q{i + 1}' for i in quantile_defaults.index])\n",
    "plt.savefig(\"Performance_Percentage_of_Defaults_by_Quantile.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, column_names):\n",
    "    # Extract feature importance (coefficients) and their absolute values\n",
    "    coefficients = model.coef_[0]\n",
    "    feature_importance = np.abs(coefficients)\n",
    "\n",
    "    # Create a DataFrame for easier visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': column_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], align='center')\n",
    "    plt.gca().invert_yaxis()  # Reverse the order for better visualization\n",
    "    plt.title('Feature Importance in Logistic Regression')\n",
    "    plt.xlabel('Importance (Absolute Coefficient Value)')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "    # Adjust layout to avoid cutting labels\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Feature Importance.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(logistic_regression, train_no_missing.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Once done with your regular data science work -> Autolog your entire notebook\n",
    "**With one single line of code Autolog all your assets (Model, Dataset, Graphs, Notes, Test Results).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnrKcTdweG-v",
    "outputId": "b04294e4-f52f-463d-b289-871628e050e7"
   },
   "outputs": [],
   "source": [
    "autolog.notebook(note= \"Baseline model logistic regression\", capture_schema_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You have logged your assets to Vectice with a single line of code.\n",
    "\n",
    "**Note**: Vectice also offers a full API to wrap any assets you want to log and give you more control: https://api-docs.vectice.com/<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¥‡ Congrats! You have learned how to successfully use Vectice to autolog your work in one line of code.<br> Easy right?<br>\n",
    "# ðŸŒŸYou can proceed back to the Vectice app to auto-document your work.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
