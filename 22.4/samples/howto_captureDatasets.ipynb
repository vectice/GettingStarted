{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Vectice\n",
    "%pip install -q vectice -U"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Paste your API token below and execute the block. (your token can be generated [here](https://app.vectice.com/account/api-keys) )   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vectice package\n",
    "import vectice\n",
    "\n",
    "# Set Vectice logging level\n",
    "import logging\n",
    "logging.getLogger(\"vectice\").setLevel(logging.WARNING)\n",
    "\n",
    "# Connect using your token API - Your token can be found here: https://app.vectice.com/account/api-keys\n",
    "conn = vectice.connect(\n",
    "    api_token='YOUR API TOKEN', \n",
    "    host='https://app.vectice.com',\n",
    "    workspace='Samples'\n",
    ")\n",
    "# Open the project\n",
    "project = conn.project(\"How To: Capture your Datasets\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Capture your dataset and their usage\n",
    "\n",
    "This sample uses data from our Vectice S3 bucket.      \n",
    "We will use boto3 as a client.   \n",
    "\n",
    "The first cell illustrates how to add dataset and tag them as origin_dataset   \n",
    "The second cell shows how to tag/attach a clean dataset ready for modeling to your project   \n",
    "The third one defines a modeling dataset that is a compound dataset (training, testing, validation)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3 import client  # Used to create a client and read from S3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "from vectice import FileDataWrapper, S3DataWrapper, GcsDataWrapper, DatasetSourceUsage\n",
    "\n",
    "s3_client = client('s3', config=Config(signature_version=UNSIGNED), region_name='us-west-1')\n",
    "\n",
    "# Data Scientist code to build data frames with data\n",
    "# ...\n",
    "\n",
    "# get the 'Identify datasets' step of the 'Data Understanding' phase\n",
    "step = project.phase(\"Data Understanding\").iteration().step(\"Identify datasets\")\n",
    "\n",
    "# Document the original datasets used for this project\n",
    "step.origin_dataset = S3DataWrapper(name=\"Stores\",s3_client=s3_client,bucket_name='vectice-examples',resource_path=\"Tutorial/ForecastTutorial/stores.csv\")\n",
    "step.origin_dataset = S3DataWrapper(name=\"Transactions\", s3_client=s3_client, bucket_name='vectice-examples', resource_path=\"Tutorial/ForecastTutorial/transactions.csv\")\n",
    "\n",
    "# Document the step and automatically attach the datasets to it. Move on the next step\n",
    "step = step.next_step(message=\"The datasets for the project have been identified as \\'stores.csv\\' and \\'transaction.csv'.\\nBoth files are located under the \\'vectice-example' S3 bucket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scientist code for data preparation, normalization, etc...\n",
    "# ...\n",
    "\n",
    "# get the 'Clean dataset' step of the 'Data Understanding' phase\n",
    "# step = project.phase(\"Data Understanding\").iteration().step(\"Clean dataset\")\n",
    "\n",
    "# Document the cleaned dataset in Vectice - using the same S3 bucket\n",
    "step.clean_dataset = S3DataWrapper(name=\"CleanDataset\", s3_client=s3_client, bucket_name='vectice-examples', resource_path=\"Tutorial/ForecastTutorial/train_clean.csv\")\n",
    "\n",
    "# Document the step, automatically attach the dataset to it and move on to the next step\n",
    "step = step.next_step(message=\"A new dataset has been created, combining both origin datasets, removing non-essentials feaures and normalized the data for modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scientist code to generate training, testing, and validation dataframes\n",
    "# ...\n",
    "\n",
    "# get the 'Clean dataset' step of the 'Data Understanding' phase\n",
    "# step = project.phase(\"Data Understanding\").iteration().step(\"Build Model\")\n",
    "\n",
    "# Define a testing, training and validation datawrapper\n",
    "train_ds = S3DataWrapper(name=\"Modeling Dataset\", s3_client=s3_client, bucket_name='vectice-examples', resource_path=\"Tutorial/ForecastTutorial/traindataset.csv\", usage=DatasetSourceUsage.TRAINING)\n",
    "test_ds = S3DataWrapper(name=\"Modeling Dataset\", s3_client=s3_client, bucket_name='vectice-examples', resource_path=\"Tutorial/ForecastTutorial/testdataset.csv\", usage=DatasetSourceUsage.TESTING)\n",
    "validate_ds = S3DataWrapper(name=\"Modeling Dataset\", s3_client=s3_client, bucket_name='vectice-examples', resource_path=\"Tutorial/ForecastTutorial/validatedataset.csv\", usage=DatasetSourceUsage.VALIDATION)\n",
    "\n",
    "# Document the cleaned dataset in Vectice - using the same S3 bucket\n",
    "step.modeling_dataset = [train_ds, test_ds, validate_ds]\n",
    "# Document the step and automatically attach the dataset\n",
    "step.close(message=\"This model iteration uses the attached modeling dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
