{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52cc6000",
   "metadata": {},
   "source": [
    "## Installing Vectice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e490e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --q vectice[git]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b651490",
   "metadata": {},
   "source": [
    "## Install optional packages for your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629fc2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --q squarify\n",
    "!pip3 install --q plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509397bd",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f7f042e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.098471,
     "end_time": "2022-01-15T09:50:45.169883",
     "exception": false,
     "start_time": "2022-01-15T09:50:45.071412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.16.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Importing the relevant libraries\n",
    "import IPython.display\n",
    "import seaborn as sns\n",
    "#import squarify\n",
    "%matplotlib inline\n",
    "# import missingno as msno\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "# D3 modules\n",
    "from IPython.display import display, HTML, Javascript\n",
    "from string import Template\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "# sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Vectice\n",
    "import vectice\n",
    "from vectice import FileDataWrapper, DatasetSourceUsage\n",
    "import logging\n",
    "import os\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baac016",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "The dataset used in this project can be found here:<br>\n",
    "* [items.csv] (https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/items.csv)<br>\n",
    "* [holidays_events.csv] (https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/holidays_events.csv)<br>\n",
    "* [stores.csv] (https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/stores.csv)<br>\n",
    "* [oil.csv] (https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/oil.csv)<br>\n",
    "* [transactions.csv] (https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/transactions.csv)<br>\n",
    "* [train_reduced.csv] (https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/train_reduced.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e49aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {'store_nbr': np.dtype('int64'),\n",
    "          'item_nbr': np.dtype('int64'),\n",
    "          'unit_sales': np.dtype('float64'),\n",
    "          'onpromotion': np.dtype('O')}\n",
    "\n",
    "items = pd.read_csv(\"items.csv\")\n",
    "holiday_events = pd.read_csv(\"holidays_events.csv\", parse_dates=['date'])\n",
    "stores = pd.read_csv(\"stores.csv\")\n",
    "oil = pd.read_csv(\"oil.csv\", parse_dates=['date'])\n",
    "transactions = pd.read_csv(\"transactions.csv\", parse_dates=['date'])\n",
    "train = pd.read_csv(\"train_reduced.csv\", parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f9e1c",
   "metadata": {
    "papermill": {
     "duration": 0.167189,
     "end_time": "2022-01-15T09:54:03.229848",
     "exception": false,
     "start_time": "2022-01-15T09:54:03.062659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe02bb5",
   "metadata": {
    "papermill": {
     "duration": 0.167783,
     "end_time": "2022-01-15T09:54:03.562802",
     "exception": false,
     "start_time": "2022-01-15T09:54:03.395019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Here we analyze the data and select the features for our model to be trained on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8bb667",
   "metadata": {
    "papermill": {
     "duration": 0.165965,
     "end_time": "2022-01-15T09:54:03.894763",
     "exception": false,
     "start_time": "2022-01-15T09:54:03.728798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Train**\n",
    "id, date, store_nbr, item_nbr, unit_scale, on_promotion\n",
    "\n",
    "**Items**\n",
    "item_nbr, family, class, perishable\n",
    "\n",
    "**Holidays_events**\n",
    "date, type, locale, locale_name, description, transferred\n",
    "\n",
    "**Stores**\n",
    "store_nbr, city, state, type, cluster\n",
    "\n",
    "**Oil**\n",
    "date, dcoilwtico\n",
    "\n",
    "**Transactions**\n",
    "date, store_nbr, transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac14ae",
   "metadata": {
    "papermill": {
     "duration": 0.168723,
     "end_time": "2022-01-15T09:54:04.231620",
     "exception": false,
     "start_time": "2022-01-15T09:54:04.062897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Selected features as inputs to the model**\n",
    "\n",
    "date, holiday.type, holidaye.locale, holiday.locale_name, holiday_transfered, store_nbr, store.city, store.state, store.type, store.cluster, transactions, item_nbr, item.family, item.class, on_promotion, perishable, dcoilwtico.\n",
    "\n",
    "**Selected features as outputs of the model**\n",
    "\n",
    "transactions per store, unit_sales per item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd3fc6",
   "metadata": {
    "papermill": {
     "duration": 0.167221,
     "end_time": "2022-01-15T09:54:04.567001",
     "exception": false,
     "start_time": "2022-01-15T09:54:04.399780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DATA pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e9b92",
   "metadata": {},
   "source": [
    "Here, we'll merge the different dataframes into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fa097ca",
   "metadata": {
    "papermill": {
     "duration": 0.279687,
     "end_time": "2022-01-15T09:54:05.347688",
     "exception": false,
     "start_time": "2022-01-15T09:54:05.068001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class prepare_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"prepare_data -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        train_stores = X[0].merge(X[1], right_on = 'store_nbr', left_on='store_nbr')\n",
    "        train_stores_oil = train_stores.merge(X[2], right_on='date', left_on='date')\n",
    "        train_stores_oil_items = train_stores_oil.merge(X[3], right_on = 'item_nbr', left_on = 'item_nbr')\n",
    "        train_stores_oil_items_transactions = train_stores_oil_items.merge(X[4], right_on = ['date', 'store_nbr'], left_on = ['date', 'store_nbr'])\n",
    "        train_stores_oil_items_transactions_hol = train_stores_oil_items_transactions.merge(X[5], right_on = 'date', left_on = 'date')\n",
    "        \n",
    "        data_df = train_stores_oil_items_transactions_hol.copy(deep = True)\n",
    "        \n",
    "        # Fill the empty values\n",
    "        data_df['onpromotion'] = data_df['onpromotion'].fillna(0)\n",
    "        # change the bool to int\n",
    "        data_df['onpromotion'] = data_df['onpromotion'].astype(int)\n",
    "        data_df['transferred'] = data_df['transferred'].astype(int)\n",
    "\n",
    "        # change the names\n",
    "        data_df.rename(columns={'type_x': 'st_type', 'type_y': 'hol_type'}, inplace=True)\n",
    "        \n",
    "        # handle date\n",
    "        data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "        data_df['date'] = data_df['date'].map(dt.datetime.toordinal)\n",
    "                \n",
    "        return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770cb9a",
   "metadata": {
    "papermill": {
     "duration": 0.167427,
     "end_time": "2022-01-15T09:54:05.681875",
     "exception": false,
     "start_time": "2022-01-15T09:54:05.514448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Custom transform for splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5bcfd",
   "metadata": {},
   "source": [
    "Here, we split dataframe into numerical values, categorical values and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fba69204",
   "metadata": {
    "papermill": {
     "duration": 0.177586,
     "end_time": "2022-01-15T09:54:06.025656",
     "exception": false,
     "start_time": "2022-01-15T09:54:05.848070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split dataframe into numerical values, categorical values and date\n",
    "class split_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"split_data -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # Get columns for each type         \n",
    "        df_ = X.drop(['date'], axis = 1)\n",
    "        cols = df_.columns\n",
    "        num_cols = df_._get_numeric_data().columns\n",
    "        cat_cols = list(set(cols) - set(num_cols))\n",
    "        \n",
    "        data_num_df = X[num_cols]\n",
    "        data_cat_df = X[cat_cols]\n",
    "        data_date_df = X['date']\n",
    "        \n",
    "        return data_num_df, data_cat_df, data_date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98b316",
   "metadata": {
    "papermill": {
     "duration": 0.165333,
     "end_time": "2022-01-15T09:54:06.357635",
     "exception": false,
     "start_time": "2022-01-15T09:54:06.192302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, we handle the missing data, apply standard scaler to numerical attributes, and convert categorical data into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3973ad6",
   "metadata": {
    "papermill": {
     "duration": 0.41475,
     "end_time": "2022-01-15T09:54:06.937863",
     "exception": false,
     "start_time": "2022-01-15T09:54:06.523113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class process_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"process_data -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        ### numerical data\n",
    "        # impute nulls in numerical attributes\n",
    "        imputer = SimpleImputer(strategy=\"mean\", copy=\"true\")\n",
    "        num_imp = imputer.fit_transform(X[0])\n",
    "        #########\n",
    "        data_num_df = pd.DataFrame(num_imp, columns=X[0].columns, index=X[0].index)\n",
    "        \n",
    "        # apply standard scaling\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data_num_df)\n",
    "        num_scaled = scaler.transform(data_num_df)\n",
    "        data_num_df = pd.DataFrame(num_scaled, columns=X[0].columns, index=X[0].index)\n",
    "        \n",
    "        ### categorical data\n",
    "        # one hot encoder\n",
    "        cat_encoder = OneHotEncoder(sparse=False)\n",
    "        data_cat_1hot = cat_encoder.fit_transform(X[1])\n",
    "        \n",
    "        # convert it to datafram with n*99 where n number of rows and 99 is no. of categories\n",
    "        data_cat_df = pd.DataFrame(data_cat_1hot, columns=cat_encoder.get_feature_names()) #, index=X[1].index)\n",
    "                \n",
    "        return data_num_df, data_cat_df, X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83573257",
   "metadata": {
    "papermill": {
     "duration": 0.176369,
     "end_time": "2022-01-15T09:54:07.611183",
     "exception": false,
     "start_time": "2022-01-15T09:54:07.434814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class join_df(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"join_df -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        ### numerical data\n",
    "        data_df = X[0].join(X[1])\n",
    "        data_df = data_df.join(X[2])\n",
    "        \n",
    "        return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61765158",
   "metadata": {},
   "source": [
    "## Push the datasets through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d99b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_processing = Pipeline([\n",
    "        ('prepare_data', prepare_data()),\n",
    "        ('split_data', split_data()),\n",
    "        ('process_data', process_data()),\n",
    "        ('join_data', join_df())\n",
    "    ])\n",
    "\n",
    "# our prepared data\n",
    "data_df = pipe_processing.fit_transform([train, stores, oil, items, transactions, holiday_events])\n",
    "data_df.to_csv(\"train_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420ffd1",
   "metadata": {},
   "source": [
    "##  Vectice Config     \n",
    "- To log your work to Vectice, you need to connect your notebook to your profile using your personal API token       \n",
    "- Click on your profile at the top right corner of the Vectice application --> API Tokens --> Create API Token       \n",
    "- Provide a name and description for the key. We recommend you name the API Token: \"Tutorial_API_Token\" to avoid having to make additional changes to the notebook.\n",
    "- Save it in  alocation accessible by this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e16536bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Tutorial_API_token.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m retail_ws \u001b[39m=\u001b[39m vectice\u001b[39m.\u001b[39;49mconnect(config\u001b[39m=\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTutorial_API_token.json\u001b[39;49m\u001b[39m\"\u001b[39;49m, workspace\u001b[39m=\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.ebarre\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/vectice/connection.py:109\u001b[0m, in \u001b[0;36mConnection.connect\u001b[0;34m(api_token, host, config, workspace, project)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mAttempts to create a client that connects to Vectice using the api_token, host, workspace or config provided. If\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39ma config is used the following keys must be set in the json file;\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39m:return: Vectice Object\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m config:\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(config, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m config_file:\n\u001b[1;32m    110\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m             config_object \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(config_file)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Tutorial_API_token.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "retail_ws = vectice.connect(config=r\"Tutorial_API_token.json\", workspace=r\".ebarre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3464f9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating our data preparation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(job=\"Data Cleaning\", project=project_id, job_type=JobType.PREPARATION, auto_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b479a12",
   "metadata": {},
   "source": [
    "### Let's get our input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b0bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ds = experiment.list_dataset_versions(dataset = \"Train trimmed data\")\n",
    "input_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d8ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inputs\n",
    "input_ds_version = experiment.get_dataset_version(version=input_ds[-1].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e98e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AWS credentials\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushing the cleaned data to S3\n",
    "data_df.to_csv(\"s3://ds-retail-project/input/Train_trimmed_cleaned.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dabfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.start(inputs = [input_ds_version])\n",
    "# Create cleaned data dataset in Vectice\n",
    "output_ds_version = experiment.vectice.create_s3_dataset(uri = \"s3://ds-retail-project/input/Train_trimmed_cleaned.csv\", name = \"Cleaned data\")\n",
    "experiment.document_run(name=\"Data Preparation\")\n",
    "experiment.complete(outputs = [output_ds_version])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4527.062431,
   "end_time": "2022-01-15T11:06:02.218444",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-15T09:50:35.156013",
   "version": "2.3.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
