{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Auto-document your work with Vectice - Quickstart\n",
    "\n",
    "This Vectice Quickstart notebook illustrates how to use Vectice auto-documentation features in a realistic business scenario. We will follow a classic but simplified model training flow to quickly show how Vectice can help you automate your **Model Documentation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"color: #383d41; background-color: #e2e3e5; border-color: #d6d8db\" role=\"alert\">\n",
    "<b> This is a quickstart project designed to showcase Vecticeâ€™s capabilities in automatically documenting notebooks. Vectice also supports more complex projects, which will be explored in upcoming tutorials.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the latest Vectice Python client library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q vectice -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "FKTpQh5HeG-s"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Samples+Data/tutorial_data.csv -q --no-check-certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Get started by configuring the Vectice autolog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites:\n",
    "Before using this notebook you will need:\n",
    "* Copy your API Key inside Vectice instructions page Paste it in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PASTE YOUR APY KEY configuration below\n",
    "\n",
    "# import vectice\n",
    "# from vectice import autolog\n",
    "\n",
    "# autolog.config( api_token = 'X91p4rl1D.gpGr8RBdw7YAOL3ZEba0X91p4rl1D2NeonklQxjMyJ6mP4vzVW', # your API Key to connect to Vectice\n",
    "#  host = 'https://app.vectice.com',  # your host info\n",
    "#  phase = 'PHA-11059', # your phase in which you want to log your work\n",
    "#  prefix = 'UB'  # A prefix for your variables name\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"color: #383d41; background-color: #e2e3e5; border-color: #d6d8db\" role=\"alert\">\n",
    "<b> Once configured, autolog automatically monitors and captures all assets from your notebookâ€”such as models, datasets, and graphsâ€”for seamless logging and documentation in Vectice.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Your Regular Data Science Work -> Not specific to Vectice\n",
    "\n",
    "In this notebook, we will work on predicting the probability of loan default using a simplified yet complete data science workflow. Here's the plan:\n",
    "\n",
    "- Dataset Loading: \n",
    "  - Load a dataset containing information about loan default applications.\n",
    "  - Select a subset of the data.\n",
    "- Data Preparation:\n",
    "  - Perform small feature engineering tasks.\n",
    "  - Apply scaling to the features.\n",
    "- Model Building and Evaluation:\n",
    "  - Build a logistic regression model to predict the probability of loan default.\n",
    "  - Evaluate the results of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a5e67831-4751-4f11-8e07-527e3e092671",
    "_uuid": "ded520f73b9e94ed47ac2e994a5fb1bcb9093d0f",
    "id": "yauz6m0neG-t"
   },
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "ihF90pQqeG-u",
    "outputId": "76973532-e7b9-4277-a95d-756af8d6878c"
   },
   "outputs": [],
   "source": [
    "# For the baseline model, we are only going to select a subset of columns that would make sense for the business, namely ['SK_ID_CURR','AMT_ANNUITY','AMT_CREDIT','AMT_INCOME_TOTAL','AMT_GOODS_PRICE','CNT_CHILDREN','CNT_FAM_MEMBERS','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\"TARGET\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "selected_columns = ['SK_ID_CURR','AMT_ANNUITY','NAME_CONTRACT_TYPE','AMT_CREDIT','AMT_INCOME_TOTAL','AMT_GOODS_PRICE','CNT_CHILDREN','CNT_FAM_MEMBERS',\n",
    "           'DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\"TARGET\"]\n",
    "\n",
    "# Training data\n",
    "path_train = path_train = \"./tutorial_data.csv\"\n",
    "application_cleaned_baseline = pd.read_csv(path_train)[selected_columns]\n",
    "app_train_feat, app_test_feat = train_test_split(application_cleaned_baseline, test_size=0.15, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate the target variable from the testing set\n",
    "\n",
    "target_variable = 'TARGET'\n",
    "app_test_feat_target = app_test_feat[target_variable]\n",
    "app_test_feat = app_test_feat.drop(target_variable, axis=1)\n",
    "\n",
    "# Print the shapes of the resulting dataframes\n",
    "print('Training data shape: ', app_train_feat.shape)\n",
    "print('Testing shape: ', app_test_feat.shape)\n",
    "print('Testing target shape: ', app_test_feat_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDIEB3zieG-u",
    "outputId": "47b3b7ca-6fea-482d-9ec2-0c7cda227e93"
   },
   "outputs": [],
   "source": [
    "app_train_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "86d1b309-5524-4298-b873-2c1c09eddec6",
    "_uuid": "1b49e667293daabffd8a4b2b6d02cf44bf6a3ba8",
    "id": "XoDuFM19eG-u"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_feat[\"CREDIT_INCOME_PERCENT\"] = (\n",
    "    app_train_feat[\"AMT_CREDIT\"] / app_train_feat[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_train_feat[\"ANNUITY_INCOME_PERCENT\"] = (\n",
    "    app_train_feat[\"AMT_ANNUITY\"] / app_train_feat[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_train_feat[\"CREDIT_TERM\"] = (\n",
    "    app_train_feat[\"AMT_ANNUITY\"] / app_train_feat[\"AMT_CREDIT\"]\n",
    ")\n",
    "app_train_feat[\"DAYS_EMPLOYED_PERCENT\"] = (\n",
    "    app_train_feat[\"DAYS_EMPLOYED\"] / app_train_feat[\"DAYS_BIRTH\"]\n",
    ")\n",
    "app_train_feat[\"NEW_SOURCES_PROD\"] = (\n",
    "    app_train_feat[\"EXT_SOURCE_2\"] * app_train_feat[\"EXT_SOURCE_3\"]\n",
    ")\n",
    "app_train_feat[\"NEW_EXT_SOURCES_MEAN\"] = app_train_feat[\n",
    "    [\"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]\n",
    "].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_test_feat[\"CREDIT_INCOME_PERCENT\"] = (\n",
    "    app_test_feat[\"AMT_CREDIT\"] / app_test_feat[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_test_feat[\"ANNUITY_INCOME_PERCENT\"] = (\n",
    "    app_test_feat[\"AMT_ANNUITY\"] / app_test_feat[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_test_feat[\"CREDIT_TERM\"] = (\n",
    "    app_test_feat[\"AMT_ANNUITY\"] / app_test_feat[\"AMT_CREDIT\"]\n",
    ")\n",
    "app_test_feat[\"DAYS_EMPLOYED_PERCENT\"] = (\n",
    "    app_test_feat[\"DAYS_EMPLOYED\"] / app_test_feat[\"DAYS_BIRTH\"]\n",
    ")\n",
    "app_test_feat[\"NEW_SOURCES_PROD\"] = (\n",
    "    app_test_feat[\"EXT_SOURCE_2\"] * app_test_feat[\"EXT_SOURCE_3\"]\n",
    ")\n",
    "app_test_feat[\"NEW_EXT_SOURCES_MEAN\"] = app_test_feat[\n",
    "    [\"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]\n",
    "].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "95627792-157e-457a-88a8-3b3875c7e1d5",
    "_uuid": "46f5bf9a6de52e270aa911ffd895e704da5426ec",
    "id": "KtEi7L-YeG-u"
   },
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0851773b-39fd-4cf0-9a66-e30adeef3e57",
    "_uuid": "6796c6dc793a08e162b6e20c6f185ef37bdf51f3",
    "id": "vf6eQZe9eG-v",
    "outputId": "88b77463-ade7-4630-8e96-f64096ca16fd"
   },
   "outputs": [],
   "source": [
    "# one-hot encoding of categorical variables\n",
    "app_train_feat = pd.get_dummies(app_train_feat)\n",
    "app_test_feat = pd.get_dummies(app_test_feat)\n",
    "train_labels = app_train_feat['TARGET']\n",
    "\n",
    "app_train_feat, app_test_feat = app_train_feat.align(app_test_feat, join = 'inner', axis = 1)\n",
    "\n",
    "app_train_feat['TARGET'] = train_labels\n",
    "\n",
    "\n",
    "app_train_feat['DAYS_EMPLOYED_ANOM'] = app_train_feat[\"DAYS_EMPLOYED\"] == 365243\n",
    "\n",
    "app_train_feat['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "\n",
    "\n",
    "app_test_feat['DAYS_EMPLOYED_ANOM'] = app_test_feat[\"DAYS_EMPLOYED\"] == 365243\n",
    "app_test_feat[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n",
    "print('Training Features shape: ', app_train_feat.shape)\n",
    "print('Testing Features shape: ', app_test_feat.shape)\n",
    "print('There are %d anomalies in the test data out of %d entries' % (app_test_feat[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test_feat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1A_lc_SeG-v"
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(features, feature_importance_values):\n",
    "\n",
    "    df = pd.DataFrame({'feature': features, 'importance': feature_importance_values}).sort_values('importance', ascending = False).reset_index()\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))),\n",
    "            df['importance_normalized'].head(15),\n",
    "            align = 'center', edgecolor = 'k')\n",
    "\n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "\n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance');\n",
    "    plt.title('Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Feature Importance.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and missing Data handling\n",
    "\n",
    "### Define the feature list - drop target and fit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izVPFh52eG-v",
    "outputId": "55cca63b-8244-4ded-8ba9-fdf1054c3d9d"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Drop the target from the training data\n",
    "if 'TARGET' in app_train_feat:\n",
    "    train_no_missing = app_train_feat.drop(columns=['TARGET'])\n",
    "\n",
    "# Separate 'SK_ID_CURR'\n",
    "train_ids = train_no_missing['SK_ID_CURR']\n",
    "test_ids = app_test_feat['SK_ID_CURR']\n",
    "\n",
    "# Drop 'SK_ID_CURR' from the features list\n",
    "train_no_missing = train_no_missing.drop(columns=['SK_ID_CURR'])\n",
    "test_no_missing = app_test_feat.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "# Define the features list without 'SK_ID_CURR'\n",
    "features = list(train_no_missing.columns)\n",
    "\n",
    "# Median imputation of missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "# Fit on the training data\n",
    "imputer.fit(train_no_missing)\n",
    "\n",
    "# Transform both training and testing data\n",
    "train_no_missing = pd.DataFrame(imputer.transform(train_no_missing), columns=features)\n",
    "test_no_missing = pd.DataFrame(imputer.transform(test_no_missing), columns=features)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "# Fit on the training data\n",
    "scaler.fit(train_no_missing)\n",
    "\n",
    "# Transform both training and testing data\n",
    "train_no_missing = pd.DataFrame(scaler.transform(train_no_missing), columns=features)\n",
    "test_no_missing = pd.DataFrame(scaler.transform(test_no_missing), columns=features)\n",
    "\n",
    "# Reattach 'SK_ID_CURR' to the DataFrames\n",
    "train_no_missing['SK_ID_CURR'] = train_ids.values\n",
    "test_no_missing['SK_ID_CURR'] = test_ids.values\n",
    "\n",
    "# Set 'SK_ID_CURR' as the index\n",
    "train_no_missing = train_no_missing.set_index('SK_ID_CURR')\n",
    "test_no_missing = test_no_missing.set_index('SK_ID_CURR')\n",
    "\n",
    "# Display the first few rows of the transformed training data\n",
    "print(train_no_missing.head())\n",
    "print(test_no_missing.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnFFx0z-eG-v"
   },
   "source": [
    "# Model building and evaluation\n",
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrCueQ_JeG-v",
    "outputId": "1220d525-1e6c-42f2-e39c-751c92d053ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Build a logistic regression model\n",
    "# Define and train the logistic regression model\n",
    "logistic_regression = LogisticRegression(random_state=50, solver='liblinear', max_iter=1000)\n",
    "features = list(train_no_missing.columns)\n",
    "# Train on the training data\n",
    "logistic_regression.fit(train_no_missing, train_labels)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = logistic_regression.predict_proba(test_no_missing)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc = roc_auc_score(app_test_feat_target.values, predictions)\n",
    "\n",
    "sorted_indices = np.argsort(predictions)[::-1]\n",
    "sorted_labels = app_test_feat_target.iloc[sorted_indices]\n",
    "\n",
    "desired_percentage = 0.25\n",
    "\n",
    "threshold_index = int(desired_percentage * len(predictions))\n",
    "threshold_probability = predictions[sorted_indices[threshold_index]]\n",
    "binary_predictions = (predictions >= threshold_probability).astype(int)\n",
    "\n",
    "# Calculate the recall at the desired percentage\n",
    "recall = recall_score(app_test_feat_target.values, binary_predictions)\n",
    "f1 = f1_score(app_test_feat_target.values, binary_predictions)\n",
    "\n",
    "# Print metrics\n",
    "metric = {\"auc\": float(roc_auc),\n",
    "          f\"recall at {desired_percentage}%\": float(recall),\n",
    "          f\"f1_score at {desired_percentage}%\": float(f1)}\n",
    "\n",
    "print(\"ROC AUC Score:\", roc_auc)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Recall Score:\", recall)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(app_test_feat_target.values, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"Performance_roc_curve.png\")\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame with predicted probabilities and true labels\n",
    "df_results = pd.DataFrame({'Probability': predictions, 'Default': app_test_feat_target.values})\n",
    "\n",
    "# Sort instances based on predicted probabilities\n",
    "df_results = df_results.sort_values(by='Probability', ascending=False)\n",
    "\n",
    "# Divide the sorted instances into quantiles (e.g., deciles)\n",
    "num_quantiles = 10\n",
    "df_results['Quantile'] = pd.qcut(df_results['Probability'], q=num_quantiles, labels=False, duplicates='drop')\n",
    "\n",
    "# Calculate the percentage of defaults in each quantile\n",
    "quantile_defaults = df_results.groupby('Quantile')['Default'].mean() * 100\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(quantile_defaults.index, quantile_defaults.values, color='blue', alpha=0.7)\n",
    "plt.xlabel('Quantile of predicted probabilities')\n",
    "plt.ylabel('Percentage of Defaults')\n",
    "plt.title('Percentage of Defaults by Quantile of Predicted Probabilities')\n",
    "plt.xticks(ticks=quantile_defaults.index, labels=[f'Q{i + 1}' for i in quantile_defaults.index])\n",
    "plt.savefig(\"Performance_Percentage_of_Defaults_by_Quantile.png\")\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, column_names):\n",
    "    # Extract feature importance (coefficients) and their absolute values\n",
    "    coefficients = model.coef_[0]\n",
    "    feature_importance = np.abs(coefficients)\n",
    "\n",
    "    # Create a DataFrame for easier visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': column_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], align='center')\n",
    "    plt.gca().invert_yaxis()  # Reverse the order for better visualization\n",
    "    plt.title('Feature Importance in Logistic Regression')\n",
    "    plt.xlabel('Importance (Absolute Coefficient Value)')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "    # Adjust layout to avoid cutting labels\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Feature Importance.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(logistic_regression, train_no_missing.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Once done with your regular data science work -> Auto document your entire notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnrKcTdweG-v",
    "outputId": "b04294e4-f52f-463d-b289-871628e050e7"
   },
   "outputs": [],
   "source": [
    "autolog.generate_doc(note= \"Baseline model logistic regression\", capture_schema_only=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
