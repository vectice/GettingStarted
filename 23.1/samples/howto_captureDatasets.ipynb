{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Vectice and other packages\n",
    "%pip install -q vectice -U\n",
    "%pip install boto3\n",
    "%pip install botocore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Paste your API token below and execute the block. (your token can be generated [here](https://app.vectice.com/account/api-keys) )   \n",
    "\n",
    "Dataset used can be found here: https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/items.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/items.csv -q --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vectice package\n",
    "import vectice\n",
    "\n",
    "\n",
    "# Connect using your token API - Your token can be found here: https://app.vectice.com/account/api-keys\n",
    "conn = vectice.connect(\n",
    "    api_token='YOUR API TOKEN', \n",
    "    host='https://app.vectice.com',\n",
    "    workspace='Samples'\n",
    ")\n",
    "# Alternate methods of connecting\n",
    "# project = vc.connect(config='~/.config/vectice-config.json')\n",
    "# provided the json file contains the \"WORKSPACE\" and \"PROJECT\" entries,\n",
    "# OR\n",
    "# project = vectice.connect(config=\"<API_key_config_name>.json\", workspace=\"ws_name\", project=\"project_name\")\n",
    "# both will return a project refrerence\n",
    "\n",
    "# Open the project\n",
    "project = conn.project(\"How To: Reporting your Milestones\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture your dataset and their usage\n",
    "\n",
    "This sample uses data from our Vectice S3 bucket. \n",
    "     \n",
    "We will use boto3 as a client.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3 import client  # Used to create a client and read from S3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "from vectice import FileDataWrapper, S3DataWrapper, GcsDataWrapper, DatasetSourceUsage\n",
    "\n",
    "s3_client = client('s3', config=Config(signature_version=UNSIGNED), region_name='us-west-1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first cell illustrates how to add dataset and tag them as origin_dataset   \n",
    "\n",
    "The second cell shows how to tag/attach a clean dataset ready for modeling to your project   \n",
    "\n",
    "The third one captures the definition of you modeling dataset (compound dataset - training, testing, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scientist code to build data frames with data\n",
    "# ...\n",
    "\n",
    "# Start an iteration of the 'Identify source datasets' step of the 'Document Dataset' phase\n",
    "step = project.phase(\"Document Dataset\").iteration().step(\"Identify source datasets\")\n",
    "\n",
    "# Retrieve a list of steps to complete\n",
    "# project.phase(\"Document Dataset\").iteration().steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document the original datasets used for this project\n",
    "# Using a S3DataWrapper for files on AWS S3:\n",
    "step.origin_dataset = S3DataWrapper(name=\"Stores\",s3_client=s3_client,bucket_name='vectice-examples',resource_path=\"Tutorial/ForecastTutorial/stores.csv\")\n",
    "step.origin_dataset = S3DataWrapper(name=\"Transactions\", s3_client=s3_client, bucket_name='vectice-examples', resource_path=\"Tutorial/ForecastTutorial/transactions.csv\")\n",
    "\n",
    "# Using a FileDataWrapper for a local file\n",
    "step.origin_dataset = FileDataWrapper(path=\"items.csv\", name=\"Items\")\n",
    "\n",
    "# More examples here: https://docs.vectice.com/python-api-docs/how-to-register-datasets\n",
    "\n",
    "# Document the step and automatically attach the datasets to it. Move on the next step\n",
    "step = step.next_step(message=\"The datasets for the project have been identified as \\'stores.csv\\' and \\'transaction.csv'.\\nBoth files are located under the \\'vectice-example' S3 bucket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scientist code for data preparation, normalization, etc...\n",
    "# ...\n",
    "\n",
    "# Document the cleaned dataset in Vectice - using the same S3 bucket\n",
    "step.clean_dataset = S3DataWrapper(name=\"CleanDataset\", s3_client=s3_client, bucket_name='vectice-examples', resource_path=\"Tutorial/ForecastTutorial/train_clean.csv\")\n",
    "\n",
    "# Document the step, automatically attach the dataset to it and move on to the next step\n",
    "step = step.next_step(message=\"As part of our standard Data Pipeline process we applied the following preparation to our datasets:\\n\"\\\n",
    "    \" - Handling of missing data\\n - Applied standard scaler to numerical attributes\\n - Converted categorical data into numerical\\n\"\\\n",
    "    \" - Split values in numerical values, categorical values, and dates.\"\\\n",
    "    \"\\n\\nWe processed our origin datasets through our data pipeline to generate a dataset ready for modeling.\\nThe dataset is ready for modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scientist code to generate training, testing, and validation dataframes\n",
    "# ...\n",
    "\n",
    "# Define a testing, training and validation datawrapper\n",
    "train_ds = S3DataWrapper(name=\"Modeling Dataset\", s3_client=s3_client, bucket_name='vectice-examples', resource_path=\"Tutorial/ForecastTutorial/traindataset.csv\", usage=DatasetSourceUsage.TRAINING)\n",
    "test_ds = S3DataWrapper(name=\"Modeling Dataset\", s3_client=s3_client, bucket_name='vectice-examples', resource_path=\"Tutorial/ForecastTutorial/testdataset.csv\", usage=DatasetSourceUsage.TESTING)\n",
    "validate_ds = S3DataWrapper(name=\"Modeling Dataset\", s3_client=s3_client, bucket_name='vectice-examples', resource_path=\"Tutorial/ForecastTutorial/validatedataset.csv\", usage=DatasetSourceUsage.VALIDATION)\n",
    "\n",
    "# Document the cleaned dataset in Vectice - using the same S3 bucket\n",
    "step.modeling_dataset = [train_ds, test_ds, validate_ds]\n",
    "# Document the step and automatically attach the dataset\n",
    "step.close(message=\"We split the dataset in a training, testing and validation datasets. 40% of the data is set aside for testing and our seed to generate repeatable datasets is 42\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
