{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4759cd4",
   "metadata": {},
   "source": [
    "# Before your start with this Tutorial\n",
    "\n",
    "**Tutorial Intention:** Providing an example of iteration and related step on a data preparation phase for you to:\n",
    "\n",
    "*   Experience the data science lifecycle using Vectice\n",
    "*   See how simple it is to connect your notebook to Vectice\n",
    "*   Learn how to structure and log your work using Vectice\n",
    "\n",
    "**Resources needed:**\n",
    "*   <b>Tutorial Project: Forecast in-store unit sales (23.1)</b> - You can find it as part of your personal workspace named after your name\n",
    "*   Vectice Webapp Documentation: https://docs.vectice.com/\n",
    "*   Vectice API documentation: https://api-docs.vectice.com/sdk/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc6000",
   "metadata": {},
   "source": [
    "## Installing Vectice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e490e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --q vectice -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b651490",
   "metadata": {},
   "source": [
    "## Install optional packages for your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629fc2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --q squarify\n",
    "%pip install --q plotly\n",
    "%pip install --q matplotlib -U "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509397bd",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f7f042e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.098471,
     "end_time": "2022-01-15T09:50:45.169883",
     "exception": false,
     "start_time": "2022-01-15T09:50:45.071412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.16.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Importing the relevant libraries\n",
    "import IPython.display\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "# D3 modules\n",
    "from IPython.display import display\n",
    "import datetime as dt\n",
    "# sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# Vectice\n",
    "import vectice\n",
    "from vectice import FileDataWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baac016",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "The dataset used in this project can be found here:<br>\n",
    "* [items.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/items.csv)<br>\n",
    "* [holidays_events.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/holidays_events.csv)<br>\n",
    "* [stores.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/stores.csv)<br>\n",
    "* [oil.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/oil.csv)<br>\n",
    "* [transactions.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/transactions.csv)<br>\n",
    "* [train_reduced.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/train_reduced.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8739da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the files locally\n",
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/items.csv -q --no-check-certificate\n",
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/holidays_events.csv -q --no-check-certificate\n",
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/stores.csv -q --no-check-certificate\n",
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/oil.csv -q --no-check-certificate\n",
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/transactions.csv -q --no-check-certificate\n",
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/train_reduced.csv -q --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e49aa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/l7f8whx5221gn40trqt3h6l80000gn/T/ipykernel_90639/862663562.py:11: FutureWarning:\n",
      "\n",
      "The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtypes = {'store_nbr': np.dtype('int64'),\n",
    "          'item_nbr': np.dtype('int64'),\n",
    "          'unit_sales': np.dtype('float64'),\n",
    "          'onpromotion': np.dtype('O')}\n",
    "\n",
    "items = pd.read_csv(\"items.csv\")\n",
    "holiday_events = pd.read_csv(\"holidays_events.csv\", parse_dates=['date'])\n",
    "stores = pd.read_csv(\"stores.csv\")\n",
    "oil = pd.read_csv(\"oil.csv\", parse_dates=['date'])\n",
    "transactions = pd.read_csv(\"transactions.csv\", parse_dates=['date'])\n",
    "train = pd.read_csv(\"train_reduced.csv\", parse_dates=['date'], error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f9e1c",
   "metadata": {
    "papermill": {
     "duration": 0.167189,
     "end_time": "2022-01-15T09:54:03.229848",
     "exception": false,
     "start_time": "2022-01-15T09:54:03.062659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe02bb5",
   "metadata": {
    "papermill": {
     "duration": 0.167783,
     "end_time": "2022-01-15T09:54:03.562802",
     "exception": false,
     "start_time": "2022-01-15T09:54:03.395019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Here we analyze the data and select the features for our model to be trained on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8bb667",
   "metadata": {
    "papermill": {
     "duration": 0.165965,
     "end_time": "2022-01-15T09:54:03.894763",
     "exception": false,
     "start_time": "2022-01-15T09:54:03.728798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Train**\n",
    "id, date, store_nbr, item_nbr, unit_scale, on_promotion\n",
    "\n",
    "**Items**\n",
    "item_nbr, family, class, perishable\n",
    "\n",
    "**Holidays_events**\n",
    "date, type, locale, locale_name, description, transferred\n",
    "\n",
    "**Stores**\n",
    "store_nbr, city, state, type, cluster\n",
    "\n",
    "**Oil**\n",
    "date, dcoilwtico\n",
    "\n",
    "**Transactions**\n",
    "date, store_nbr, transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac14ae",
   "metadata": {
    "papermill": {
     "duration": 0.168723,
     "end_time": "2022-01-15T09:54:04.231620",
     "exception": false,
     "start_time": "2022-01-15T09:54:04.062897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Selected features as inputs to the model**\n",
    "\n",
    "date, holiday.type, holidaye.locale, holiday.locale_name, holiday_transfered, store_nbr, store.city, store.state, store.type, store.cluster, transactions, item_nbr, item.family, item.class, on_promotion, perishable, dcoilwtico.\n",
    "\n",
    "**Selected features as outputs of the model**\n",
    "\n",
    "transactions per store, unit_sales per item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd3fc6",
   "metadata": {
    "papermill": {
     "duration": 0.167221,
     "end_time": "2022-01-15T09:54:04.567001",
     "exception": false,
     "start_time": "2022-01-15T09:54:04.399780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DATA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fa097ca",
   "metadata": {
    "papermill": {
     "duration": 0.279687,
     "end_time": "2022-01-15T09:54:05.347688",
     "exception": false,
     "start_time": "2022-01-15T09:54:05.068001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class prepare_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"prepare_data -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        train_stores = X[0].merge(X[1], right_on = 'store_nbr', left_on='store_nbr')\n",
    "        train_stores_oil = train_stores.merge(X[2], right_on='date', left_on='date')\n",
    "        train_stores_oil_items = train_stores_oil.merge(X[3], right_on = 'item_nbr', left_on = 'item_nbr')\n",
    "        train_stores_oil_items_transactions = train_stores_oil_items.merge(X[4], right_on = ['date', 'store_nbr'], left_on = ['date', 'store_nbr'])\n",
    "        train_stores_oil_items_transactions_hol = train_stores_oil_items_transactions.merge(X[5], right_on = 'date', left_on = 'date')\n",
    "        \n",
    "        data_df = train_stores_oil_items_transactions_hol.copy(deep = True)\n",
    "        \n",
    "        # Fill the empty values\n",
    "        data_df['onpromotion'] = data_df['onpromotion'].fillna(0)\n",
    "        # change the bool to int\n",
    "        data_df['onpromotion'] = data_df['onpromotion'].astype(int)\n",
    "        data_df['transferred'] = data_df['transferred'].astype(int)\n",
    "\n",
    "        # change the names\n",
    "        data_df.rename(columns={'type_x': 'st_type', 'type_y': 'hol_type'}, inplace=True)\n",
    "        \n",
    "        # handle date\n",
    "        data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "        data_df['date'] = data_df['date'].map(dt.datetime.toordinal)\n",
    "                \n",
    "        return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770cb9a",
   "metadata": {
    "papermill": {
     "duration": 0.167427,
     "end_time": "2022-01-15T09:54:05.681875",
     "exception": false,
     "start_time": "2022-01-15T09:54:05.514448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Custom transform for splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5bcfd",
   "metadata": {},
   "source": [
    "Here, we split dataframe into numerical values, categorical values and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fba69204",
   "metadata": {
    "papermill": {
     "duration": 0.177586,
     "end_time": "2022-01-15T09:54:06.025656",
     "exception": false,
     "start_time": "2022-01-15T09:54:05.848070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split dataframe into numerical values, categorical values and date\n",
    "class split_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"split_data -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # Get columns for each type         \n",
    "        df_ = X.drop(['date'], axis = 1)\n",
    "        cols = df_.columns\n",
    "        num_cols = df_._get_numeric_data().columns\n",
    "        cat_cols = list(set(cols) - set(num_cols))\n",
    "        \n",
    "        data_num_df = X[num_cols]\n",
    "        data_cat_df = X[cat_cols]\n",
    "        data_date_df = X['date']\n",
    "        \n",
    "        return data_num_df, data_cat_df, data_date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98b316",
   "metadata": {
    "papermill": {
     "duration": 0.165333,
     "end_time": "2022-01-15T09:54:06.357635",
     "exception": false,
     "start_time": "2022-01-15T09:54:06.192302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, we handle the missing data, apply standard scaler to numerical attributes, and convert categorical data into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3973ad6",
   "metadata": {
    "papermill": {
     "duration": 0.41475,
     "end_time": "2022-01-15T09:54:06.937863",
     "exception": false,
     "start_time": "2022-01-15T09:54:06.523113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class process_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"process_data -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        ### numerical data\n",
    "        # impute nulls in numerical attributes\n",
    "        imputer = SimpleImputer(strategy=\"mean\", copy=True)\n",
    "        num_imp = imputer.fit_transform(X[0])\n",
    "        #########\n",
    "        data_num_df = pd.DataFrame(num_imp, columns=X[0].columns, index=X[0].index)\n",
    "        \n",
    "        # apply standard scaling\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data_num_df)\n",
    "        num_scaled = scaler.transform(data_num_df)\n",
    "        data_num_df = pd.DataFrame(num_scaled, columns=X[0].columns, index=X[0].index)\n",
    "        \n",
    "        ### categorical data\n",
    "        # one hot encoder\n",
    "        cat_encoder = OneHotEncoder(sparse=False)\n",
    "        data_cat_1hot = cat_encoder.fit_transform(X[1])\n",
    "        \n",
    "        # convert it to dataframe with n*99 where n number of rows and 99 is no. of categories\n",
    "        data_cat_df = pd.DataFrame(data_cat_1hot, columns=cat_encoder.get_feature_names_out()) #, index=X[1].index)\n",
    "                \n",
    "        return data_num_df, data_cat_df, X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83573257",
   "metadata": {
    "papermill": {
     "duration": 0.176369,
     "end_time": "2022-01-15T09:54:07.611183",
     "exception": false,
     "start_time": "2022-01-15T09:54:07.434814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class join_df(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"join_df -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        ### numerical data\n",
    "        data_df = X[0].join(X[1])\n",
    "        data_df = data_df.join(X[2])\n",
    "        \n",
    "        return data_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61765158",
   "metadata": {},
   "source": [
    "# Push the datasets through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82d99b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data -> init\n",
      "split_data -> init\n",
      "process_data -> init\n",
      "join_df -> init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:808: FutureWarning:\n",
      "\n",
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe_processing = Pipeline([\n",
    "        ('prepare_data', prepare_data()),\n",
    "        ('split_data', split_data()),\n",
    "        ('process_data', process_data()),\n",
    "        ('join_data', join_df())\n",
    "    ])\n",
    "\n",
    "# our prepared data\n",
    "data_df = pipe_processing.fit_transform([train, stores, oil, items, transactions, holiday_events])\n",
    "data_df.to_csv(\"train_clean.csv\") #this is the dataset that will be split into a training, testing, and validation dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b420ffd1",
   "metadata": {},
   "source": [
    "#  Document in Vectice   \n",
    "- To log your work to Vectice, you need to connect your notebook to your profile using your personal API token       \n",
    "- Click on your profile at the top right corner of the Vectice application --> API Tokens --> Create API Token       \n",
    "- Provide a name and description for the key. We recommend you name the API Token: \"Tutorial_API_Token\" to avoid having to make additional changes to the notebook.\n",
    "- Save it in a location accessible by this code\n",
    "- #### If you are viewing this notebook in Google Colab, click the folder icon on the left bar and upload the file\n",
    "\n",
    "#### Update the workspace name below to match the workspace name your project is in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e16536bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/02/17 11:39:21 INFO vectice.connection: Vectice successfully connected.\n",
      "2023/02/17 11:39:23 INFO vectice.connection: Your current workspace: .Retail Ops and project: Corp Forecast in-store unit sales\n",
      "2023/02/17 11:39:24 INFO vectice.connection: Assets with Latest Activity   Asset Type    Name\n",
      "2023/02/17 11:39:24 INFO vectice.connection: Assets with Latest Activity   Project       'Corp Forecast in-store unit sales'\n",
      "2023/02/17 11:39:24 INFO vectice.connection: Assets with Latest Activity   Phase         'Data Understanding'\n",
      "2023/02/17 11:39:24 INFO vectice.connection: Assets with Latest Activity   Iteration      4\n",
      "2023/02/17 11:39:24 INFO vectice.connection: Assets with Latest Activity   Step          ''\n"
     ]
    }
   ],
   "source": [
    "my_vectice = vectice.connect(config=\"Tutorial_API_token.json\")\n",
    "my_workspace = my_vectice.workspace(\"YOUR WORKSPACE NAME\") # replace workspace name\n",
    "my_project = my_workspace.project(\"Tutorial Project: Forecast in store unit sales (23.1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3464f9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Capture milestones for the Data Preparation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "984f0655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/02/17 11:39:26 INFO vectice.models.phase: Iteration number 1 (id 4570) successfully retrieved.\n",
      "2023/02/17 11:39:27 INFO vectice.models.datasource.datawrapper.file_data_wrapper: File: items_2.csv wrapped successfully.\n",
      "2023/02/17 11:39:27 INFO vectice.models.phase: Iteration number 1 (id 4570) successfully retrieved.\n",
      "2023/02/17 11:39:29 INFO vectice.models.step: Code captured and will be linked to asset.\n",
      "2023/02/17 11:39:34 INFO vectice.models.git_version: Code captured the following changed files; .gitignore, 22.4/samples/SimpleProject_HelloWorld.ipynb, 23.1/samples/howto_captureDatasets.ipynb, 23.1/samples/howto_captureModels.ipynb, 23.1/tutorial/Data_Preparation.ipynb\n",
      "2023/02/17 11:39:34 INFO vectice.api.client: Successfully registered Dataset(name='Items origin', id=22604, version='Version 2', type=ORIGIN).\n",
      "2023/02/17 11:39:34 INFO vectice.models.step: Dataset: Items origin with Version: Version 2 already exists.\n",
      "2023/02/17 11:39:36 INFO vectice.models.step: Successfully added Dataset(name='Items origin', id=22604, version='Version 2', type=ORIGIN) to Select Data\n",
      "2023/02/17 11:39:36 INFO vectice.models.datasource.datawrapper.file_data_wrapper: File: holidays_events.csv wrapped successfully.\n",
      "2023/02/17 11:39:36 INFO vectice.models.phase: Iteration number 1 (id 4570) successfully retrieved.\n",
      "2023/02/17 11:39:38 WARNING vectice.models.step: The code commit exists already.\n",
      "2023/02/17 11:39:39 INFO vectice.api.client: Successfully registered Dataset(name='Holiday origin', id=22590, version='Version 1', type=ORIGIN).\n",
      "2023/02/17 11:39:39 INFO vectice.models.step: Dataset: Holiday origin with Version: Version 1 already exists.\n",
      "2023/02/17 11:39:41 INFO vectice.models.step: Successfully added Dataset(name='Holiday origin', id=22590, version='Version 1', type=ORIGIN) to Select Data\n",
      "2023/02/17 11:39:41 INFO vectice.models.datasource.datawrapper.file_data_wrapper: File: stores_2.csv wrapped successfully.\n",
      "2023/02/17 11:39:41 INFO vectice.models.phase: Iteration number 1 (id 4570) successfully retrieved.\n",
      "2023/02/17 11:39:43 WARNING vectice.models.step: The code commit exists already.\n",
      "2023/02/17 11:39:43 INFO vectice.api.client: Successfully registered Dataset(name='Stores origin', id=22605, version='Version 2', type=ORIGIN).\n",
      "2023/02/17 11:39:43 INFO vectice.models.step: Dataset: Stores origin with Version: Version 2 already exists.\n",
      "2023/02/17 11:39:45 INFO vectice.models.step: Successfully added Dataset(name='Stores origin', id=22605, version='Version 2', type=ORIGIN) to Select Data\n",
      "2023/02/17 11:39:45 INFO vectice.models.datasource.datawrapper.file_data_wrapper: File: oil.csv wrapped successfully.\n",
      "2023/02/17 11:39:45 INFO vectice.models.phase: Iteration number 1 (id 4570) successfully retrieved.\n",
      "2023/02/17 11:39:47 WARNING vectice.models.step: The code commit exists already.\n",
      "2023/02/17 11:39:48 INFO vectice.api.client: Successfully registered Dataset(name='Oil origin', id=22592, version='Version 1', type=ORIGIN).\n",
      "2023/02/17 11:39:48 INFO vectice.models.step: Dataset: Oil origin with Version: Version 1 already exists.\n",
      "2023/02/17 11:39:49 INFO vectice.models.step: Successfully added Dataset(name='Oil origin', id=22592, version='Version 1', type=ORIGIN) to Select Data\n",
      "2023/02/17 11:39:49 INFO vectice.models.datasource.datawrapper.file_data_wrapper: File: transactions.csv wrapped successfully.\n",
      "2023/02/17 11:39:50 INFO vectice.models.phase: Iteration number 1 (id 4570) successfully retrieved.\n",
      "2023/02/17 11:39:52 WARNING vectice.models.step: The code commit exists already.\n",
      "2023/02/17 11:39:52 INFO vectice.api.client: Successfully registered Dataset(name='Transactions origin', id=22593, version='Version 1', type=ORIGIN).\n",
      "2023/02/17 11:39:52 INFO vectice.models.step: Dataset: Transactions origin with Version: Version 1 already exists.\n",
      "2023/02/17 11:39:53 INFO vectice.models.step: Successfully added Dataset(name='Transactions origin', id=22593, version='Version 1', type=ORIGIN) to Select Data\n",
      "2023/02/17 11:39:54 INFO vectice.models.step: 'Select Data' was successfully closed.\n",
      "2023/02/17 11:39:55 INFO vectice.models.step: Next step : Step(name='Clean data', id=17239, description='Remove/replace outliers, fill in missing values, etc.', completed=False)\n",
      "2023/02/17 11:39:55 INFO vectice.models.step: 'Clean data' was successfully closed.\n",
      "2023/02/17 11:39:56 INFO vectice.models.step: Next step : Step(name='Construct Data', id=17240, description='Create new data features to provide more insights and better predictions. For example re-group categorical variables, create new variables by combining multiple variables, etc...', completed=False)\n",
      "2023/02/17 11:39:56 INFO vectice.models.step: 'Construct Data' was successfully closed.\n",
      "2023/02/17 11:39:57 INFO vectice.models.step: Next step : Step(name='Integrate Data', id=17243, description='Merge Data from various sources into a single dataset.', completed=False)\n",
      "2023/02/17 11:39:57 INFO vectice.models.step: 'Integrate Data' was successfully closed.\n",
      "2023/02/17 11:39:58 INFO vectice.models.datasource.datawrapper.file_data_wrapper: File: train_clean.csv wrapped successfully.\n",
      "2023/02/17 11:39:58 INFO vectice.models.phase: Iteration number 1 (id 4570) successfully retrieved.\n",
      "2023/02/17 11:39:59 WARNING vectice.models.step: The Step Clean data is completed!\n",
      "2023/02/17 11:39:59 WARNING vectice.models.step: The Step Construct Data is completed!\n",
      "2023/02/17 11:39:59 WARNING vectice.models.step: The Step Select Data is completed!\n",
      "2023/02/17 11:39:59 WARNING vectice.models.step: The Step Integrate Data is completed!\n",
      "2023/02/17 11:40:00 WARNING vectice.models.step: The code commit exists already.\n",
      "2023/02/17 11:40:02 INFO vectice.api.client: Successfully registered Dataset(name='Clean&Augmented_Dataset', id=22606, version='Version 1', type=CLEAN).\n",
      "2023/02/17 11:40:03 INFO vectice.models.step: Successfully added Dataset(name='Clean&Augmented_Dataset', id=22606, version='Version 1', type=CLEAN) to Format Data\n",
      "2023/02/17 11:40:04 INFO vectice.models.step: 'Format Data' was successfully closed.\n"
     ]
    }
   ],
   "source": [
    "# Get the phase for Data Preparation \n",
    "project_dp = my_project.phase(\"Data Preparation\")   \n",
    "\n",
    "# Let's start a new iteration (or get the curently opened iteration)\n",
    "project_iter = project_dp.iteration()\n",
    "\n",
    "# Let's select the first step\n",
    "step = project_iter.step('Select Data')\n",
    "\n",
    "# Provide context into the origin datasets by attaching them to the step\n",
    "step.origin_dataset = FileDataWrapper(path=\"items.csv\", name=\"Items origin\")\n",
    "step.origin_dataset = FileDataWrapper(path=\"holidays_events.csv\", name=\"Holiday origin\")\n",
    "step.origin_dataset = FileDataWrapper(path=\"stores.csv\", name=\"Stores origin\")\n",
    "step.origin_dataset = FileDataWrapper(path=\"oil.csv\", name=\"Oil origin\")\n",
    "step.origin_dataset = FileDataWrapper(path=\"transactions.csv\", name=\"Transactions origin\")\n",
    "\n",
    "# Done with this step, let's close it and get the next step - all in one line\n",
    "# Alternatively you can explicitly close the step and manually retrieve the next one\n",
    "step = step.next_step(message=\"The datasets for the project have been identified as:\")\n",
    "\n",
    "# Log in findings/comments for this milestone, close the step and capture the next one\n",
    "msg = \"As part of our standard Data Pipeline process we applied the following preparation to our datasets:\\n - Handling of missing data\\n - Applied standard scaler to numerical attributes\\n - Converted categorical data into numerical\\n - Split values in numerical values, categorical values, and dates\"\n",
    "step = step.next_step(message=msg)\n",
    "\n",
    "# Log in findings/comments for this milestone, close the step and capture the next one\n",
    "step = step.next_step(message=\"We selected \\\"unit sales\\\" as our model target.\\nThe features used in this model are:\\n - date\\n - holiday.type\\n - holidaye.locale\\n - holiday.locale_name\\n - holiday_transfered\\n - store_nbr\\n - store.city\\n - store.state\\n - store.type\\n - store.cluster\\n - transactions\\n - item_nbr\\n - item.family\\n - item.class\\n - on_promotion\\n - perishable\\n - dcoilwtico\")\n",
    "\n",
    "# Log in findings/comments for this milestone, close the step and capture the next one\n",
    "msg = \"We processed our origin datasets through our data pipeline to generate a dataset ready for modeling.\\n\"\n",
    "msg += f\"The resulting modeling datasets contains {data_df.shape[0]} observations and {data_df.shape[1]} features.\\n\"\n",
    "msg += \"The dataset is ready to be split for modeling.\"\n",
    "step.close(message=msg)\n",
    "\n",
    "# Get the Format data step. Since we used the close() method above we need to specify the step name\n",
    "step = project_iter.step(\"Format Data\")\n",
    "step.clean_dataset = FileDataWrapper(path=\"train_clean.csv\", name=\"Clean&Augmented_Dataset\")\n",
    "# Log in findings/comments for this milestone and close the step\n",
    "step.close(message=\"We generated a dataset ready for modeling. We also created a data pipeline to make this process repeatable.\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4527.062431,
   "end_time": "2022-01-15T11:06:02.218444",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-15T09:50:35.156013",
   "version": "2.3.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
