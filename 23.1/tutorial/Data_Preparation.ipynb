{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d76614",
   "metadata": {},
   "source": [
    "# Before your start with this Tutorial\n",
    "\n",
    "**Tutorial Intention:** Providing an example of iteration and related step on a modeling phase for you to:\n",
    "\n",
    "*   Experience the data science lifecycle using Vectice\n",
    "*   See how simple it is to connect your notebook to Vectice\n",
    "*   Learn how to structure and log your work using Vectice\n",
    "\n",
    "**Resources needed:**\n",
    "*   <b>Tutorial Project: Forecast in-store unit sales (23.1)</b> - You can find it as part of your personal workspace       \n",
    "\n",
    "**Other resources:**\n",
    "*   Vectice Webapp Documentation: https://docs.vectice.com/\n",
    "*   Vectice API documentation: https://api-docs.vectice.com/sdk/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c357fd6",
   "metadata": {},
   "source": [
    "# 1. Getting Started         \n",
    "\n",
    "**First, we need to install and authenticate ourselves to the Vectice server. Before proceeding further:**\n",
    "*   Visit the Vectice app (https://app.vectice.com/account/api-keys) to create and download an API token, name the file as \"My Token\"\n",
    "*   Upload the file to Colab by clicking on the \"folder\" icon on the left-hand taskbar and selecting \"Upload to Session Storage\"\n",
    "\n",
    "* If you then execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e490e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --q vectice==23.1.7.2\n",
    "import vectice as vct\n",
    "\n",
    "vec = vct.connect(config=\"My Token.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97bf3319",
   "metadata": {},
   "source": [
    "#### You have successfully installed Vectice in your notebook and connected to your instance. \n",
    "#### Wasn't that easy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b651490",
   "metadata": {},
   "source": [
    "## Install optional packages for your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629fc2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --q squarify\n",
    "%pip install --q plotly\n",
    "%pip install --q matplotlib -U "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509397bd",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f042e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.098471,
     "end_time": "2022-01-15T09:50:45.169883",
     "exception": false,
     "start_time": "2022-01-15T09:50:45.071412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Importing the relevant libraries\n",
    "import IPython.display\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "# D3 modules\n",
    "from IPython.display import display\n",
    "import datetime as dt\n",
    "# sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# Vectice\n",
    "import vectice\n",
    "from vectice import FileResource"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6baac016",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "The dataset used in this project can be found here:<br>\n",
    "* [items.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/items.csv)<br>\n",
    "* [holidays_events.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/holidays_events.csv)<br>\n",
    "* [stores.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/stores.csv)<br>\n",
    "* [oil.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/oil.csv)<br>\n",
    "* [transactions.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/transactions.csv)<br>\n",
    "* [train_reduced.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/train_reduced.csv)\n",
    "\n",
    "Excute the cell below to download the files locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8739da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the files locally\n",
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/items.csv -q --no-check-certificate\n",
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/holidays_events.csv -q --no-check-certificate\n",
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/stores.csv -q --no-check-certificate\n",
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/oil.csv -q --no-check-certificate\n",
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/transactions.csv -q --no-check-certificate\n",
    "!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/train_reduced.csv -q --no-check-certificate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a33948b8",
   "metadata": {},
   "source": [
    "#### Great! Let's build dataframes from the file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {'store_nbr': np.dtype('int64'),\n",
    "          'item_nbr': np.dtype('int64'),\n",
    "          'unit_sales': np.dtype('float64'),\n",
    "          'onpromotion': np.dtype('O')}\n",
    "\n",
    "items = pd.read_csv(\"items.csv\")\n",
    "holiday_events = pd.read_csv(\"holidays_events.csv\", parse_dates=['date'])\n",
    "stores = pd.read_csv(\"stores.csv\")\n",
    "oil = pd.read_csv(\"oil.csv\", parse_dates=['date'])\n",
    "transactions = pd.read_csv(\"transactions.csv\", parse_dates=['date'])\n",
    "train = pd.read_csv(\"train_reduced.csv\", parse_dates=['date'], on_bad_lines='warn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f9e1c",
   "metadata": {
    "papermill": {
     "duration": 0.167189,
     "end_time": "2022-01-15T09:54:03.229848",
     "exception": false,
     "start_time": "2022-01-15T09:54:03.062659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe02bb5",
   "metadata": {
    "papermill": {
     "duration": 0.167783,
     "end_time": "2022-01-15T09:54:03.562802",
     "exception": false,
     "start_time": "2022-01-15T09:54:03.395019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Here we analyze the data and select the features for our model to be trained on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8bb667",
   "metadata": {
    "papermill": {
     "duration": 0.165965,
     "end_time": "2022-01-15T09:54:03.894763",
     "exception": false,
     "start_time": "2022-01-15T09:54:03.728798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Train**\n",
    "id, date, store_nbr, item_nbr, unit_scale, on_promotion\n",
    "\n",
    "**Items**\n",
    "item_nbr, family, class, perishable\n",
    "\n",
    "**Holidays_events**\n",
    "date, type, locale, locale_name, description, transferred\n",
    "\n",
    "**Stores**\n",
    "store_nbr, city, state, type, cluster\n",
    "\n",
    "**Oil**\n",
    "date, dcoilwtico\n",
    "\n",
    "**Transactions**\n",
    "date, store_nbr, transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac14ae",
   "metadata": {
    "papermill": {
     "duration": 0.168723,
     "end_time": "2022-01-15T09:54:04.231620",
     "exception": false,
     "start_time": "2022-01-15T09:54:04.062897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Selected features as inputs to the model**\n",
    "\n",
    "date, holiday.type, holidaye.locale, holiday.locale_name, holiday_transfered, store_nbr, store.city, store.state, store.type, store.cluster, transactions, item_nbr, item.family, item.class, on_promotion, perishable, dcoilwtico.\n",
    "\n",
    "**Selected features as outputs of the model**\n",
    "\n",
    "transactions per store, unit_sales per item"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53bd3fc6",
   "metadata": {
    "papermill": {
     "duration": 0.167221,
     "end_time": "2022-01-15T09:54:04.567001",
     "exception": false,
     "start_time": "2022-01-15T09:54:04.399780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DATA pipeline\n",
    "\n",
    "#### The next four cells are functions used as part of our Data Pipeline process.\n",
    "#### Nothing Vectice specific, just boiler plate code.\n",
    "#### Feel free to look through it but no need to spend time on it.\n",
    "#### Go ahead and jump ahead to \"Documeetn in Vection\" section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa097ca",
   "metadata": {
    "papermill": {
     "duration": 0.279687,
     "end_time": "2022-01-15T09:54:05.347688",
     "exception": false,
     "start_time": "2022-01-15T09:54:05.068001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class prepare_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"prepare_data -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        train_stores = X[0].merge(X[1], right_on = 'store_nbr', left_on='store_nbr')\n",
    "        train_stores_oil = train_stores.merge(X[2], right_on='date', left_on='date')\n",
    "        train_stores_oil_items = train_stores_oil.merge(X[3], right_on = 'item_nbr', left_on = 'item_nbr')\n",
    "        train_stores_oil_items_transactions = train_stores_oil_items.merge(X[4], right_on = ['date', 'store_nbr'], left_on = ['date', 'store_nbr'])\n",
    "        train_stores_oil_items_transactions_hol = train_stores_oil_items_transactions.merge(X[5], right_on = 'date', left_on = 'date')\n",
    "        \n",
    "        data_df = train_stores_oil_items_transactions_hol.copy(deep = True)\n",
    "        \n",
    "        # Fill the empty values\n",
    "        data_df['onpromotion'] = data_df['onpromotion'].fillna(0)\n",
    "        # change the bool to int\n",
    "        data_df['onpromotion'] = data_df['onpromotion'].astype(int)\n",
    "        data_df['transferred'] = data_df['transferred'].astype(int)\n",
    "\n",
    "        # change the names\n",
    "        data_df.rename(columns={'type_x': 'st_type', 'type_y': 'hol_type'}, inplace=True)\n",
    "        \n",
    "        # handle date\n",
    "        data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "        data_df['date'] = data_df['date'].map(dt.datetime.toordinal)\n",
    "                \n",
    "        return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770cb9a",
   "metadata": {
    "papermill": {
     "duration": 0.167427,
     "end_time": "2022-01-15T09:54:05.681875",
     "exception": false,
     "start_time": "2022-01-15T09:54:05.514448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Custom transform for splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5bcfd",
   "metadata": {},
   "source": [
    "Here, we split dataframe into numerical values, categorical values and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba69204",
   "metadata": {
    "papermill": {
     "duration": 0.177586,
     "end_time": "2022-01-15T09:54:06.025656",
     "exception": false,
     "start_time": "2022-01-15T09:54:05.848070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split dataframe into numerical values, categorical values and date\n",
    "class split_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"split_data -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # Get columns for each type         \n",
    "        df_ = X.drop(['date'], axis = 1)\n",
    "        cols = df_.columns\n",
    "        num_cols = df_._get_numeric_data().columns\n",
    "        cat_cols = list(set(cols) - set(num_cols))\n",
    "        \n",
    "        data_num_df = X[num_cols]\n",
    "        data_cat_df = X[cat_cols]\n",
    "        data_date_df = X['date']\n",
    "        \n",
    "        return data_num_df, data_cat_df, data_date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98b316",
   "metadata": {
    "papermill": {
     "duration": 0.165333,
     "end_time": "2022-01-15T09:54:06.357635",
     "exception": false,
     "start_time": "2022-01-15T09:54:06.192302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, we handle the missing data, apply standard scaler to numerical attributes, and convert categorical data into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3973ad6",
   "metadata": {
    "papermill": {
     "duration": 0.41475,
     "end_time": "2022-01-15T09:54:06.937863",
     "exception": false,
     "start_time": "2022-01-15T09:54:06.523113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class process_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"process_data -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        ### numerical data\n",
    "        # impute nulls in numerical attributes\n",
    "        imputer = SimpleImputer(strategy=\"mean\", copy=True)\n",
    "        num_imp = imputer.fit_transform(X[0])\n",
    "        #########\n",
    "        data_num_df = pd.DataFrame(num_imp, columns=X[0].columns, index=X[0].index)\n",
    "        \n",
    "        # apply standard scaling\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data_num_df)\n",
    "        num_scaled = scaler.transform(data_num_df)\n",
    "        data_num_df = pd.DataFrame(num_scaled, columns=X[0].columns, index=X[0].index)\n",
    "        \n",
    "        ### categorical data\n",
    "        # one hot encoder\n",
    "        cat_encoder = OneHotEncoder(sparse=False)\n",
    "        data_cat_1hot = cat_encoder.fit_transform(X[1])\n",
    "        \n",
    "        # convert it to dataframe with n*99 where n number of rows and 99 is no. of categories\n",
    "        data_cat_df = pd.DataFrame(data_cat_1hot, columns=cat_encoder.get_feature_names_out()) #, index=X[1].index)\n",
    "                \n",
    "        return data_num_df, data_cat_df, X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83573257",
   "metadata": {
    "papermill": {
     "duration": 0.176369,
     "end_time": "2022-01-15T09:54:07.611183",
     "exception": false,
     "start_time": "2022-01-15T09:54:07.434814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class join_df(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"join_df -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        ### numerical data\n",
    "        data_df = X[0].join(X[1])\n",
    "        data_df = data_df.join(X[2])\n",
    "        \n",
    "        return data_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61765158",
   "metadata": {},
   "source": [
    "# Push the datasets through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d99b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_processing = Pipeline([\n",
    "        ('prepare_data', prepare_data()),\n",
    "        ('split_data', split_data()),\n",
    "        ('process_data', process_data()),\n",
    "        ('join_data', join_df())\n",
    "    ])\n",
    "\n",
    "# our prepared data\n",
    "data_df = pipe_processing.fit_transform([train, stores, oil, items, transactions, holiday_events])\n",
    "data_df.to_csv(\"train_clean.csv\") #this is the dataset that will be split into a training, testing, and validation dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b420ffd1",
   "metadata": {},
   "source": [
    "#### Navigate your way to your personal workspace, get the tutorial project and start an iteration of the 'Data Prep' phase. Go ahead and execute the cell below to navigate to your workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16536bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_iter = vec.workspaces[0].project(8055).phase(\"Data Preparation\").iteration() # Make sure to use the proper project ID or name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3464f9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Capture milestones for the Data Preparation phase\n",
    "\n",
    "#### Execute the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide context into the origin datasets by attaching them to the step\n",
    "active_iter.step_select_data = vct.Dataset.origin(name=\"Items origin\",resource=FileResource(path=\"items.csv\"))\n",
    "active_iter.step_select_data += vct.Dataset.origin(name=\"Holiday origin\",resource=FileResource(path=\"holidays_events.csv\"))\n",
    "active_iter.step_select_data += vct.Dataset.origin(name=\"Stores origin\",resource=FileResource(path=\"stores.csv\"))\n",
    "active_iter.step_select_data += vct.Dataset.origin(name=\"Oil origin\",resource=FileResource(path=\"oil.csv\"))\n",
    "active_iter.step_select_data += vct.Dataset.origin(name=\"Transactions origin\",resource=FileResource(path=\"transactions.csv\"))\n",
    "\n",
    "active_iter.step_select_data = \"The datasets for the project have been identified\"\n",
    "\n",
    "# Great we have documented the datasets used.\n",
    "\n",
    "# Let's move on the next step...documenting our data pipeline\n",
    "# Log in findings/comments for this milestone\n",
    "msg = \"As part of our standard Data Pipeline process we applied the following preparation to our datasets:\\n - Handling of missing data\\n - Applied standard scaler to numerical attributes\\n - Converted categorical data into numerical\\n - Split values in numerical values, categorical values, and dates\"\n",
    "active_iter.step_clean_data = msg\n",
    "\n",
    "# Log in findings/comments for this milestone, close the step and capture the next one\n",
    "active_iter.step_construct_data = \"We selected \\\"unit sales\\\" as our model target.\\nThe features used in this model are:\\n - date\\n - holiday.type\\n - holidaye.locale\\n - holiday.locale_name\\n - holiday_transfered\\n - store_nbr\\n - store.city\\n - store.state\\n - store.type\\n - store.cluster\\n - transactions\\n - item_nbr\\n - item.family\\n - item.class\\n - on_promotion\\n - perishable\\n - dcoilwtico\"\n",
    "\n",
    "# Log in findings/comments for this milestone, close the step and capture the next one'\n",
    "# and attach the clean dataset generated\n",
    "msg = \"We processed our origin datasets through our data pipeline to generate a dataset ready for modeling.\\n\"\n",
    "msg += f\"The resulting modeling datasets contains {data_df.shape[0]} observations and {data_df.shape[1]} features.\\n\"\n",
    "msg += \"The dataset is ready to be split for modeling.\"\n",
    "active_iter.step_integrate_data = vct.Dataset.clean(name=\"Clean&Augmented_Dataset\",resource=FileResource(path=\"train_clean.csv\"))\n",
    "active_iter.step_integrate_data = msg\n",
    "\n",
    "# Log in our activity\n",
    "active_iter.step_format_data = \"We generated a dataset ready for modeling. We also created a data pipeline to make this process repeatable.\"\n",
    "active_iter.complete()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a6c27c3",
   "metadata": {},
   "source": [
    "#### The cell above creates a new phase iteration and document all the step in our data pipeline process"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4527.062431,
   "end_time": "2022-01-15T11:06:02.218444",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-15T09:50:35.156013",
   "version": "2.3.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
